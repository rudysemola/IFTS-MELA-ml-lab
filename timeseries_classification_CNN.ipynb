{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rudysemola/IFTS-MELA-ml-lab/blob/main/timeseries_classification_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGMtr1UfCYzI"
      },
      "source": [
        "# Timeseries classification: Classificazione delle serie temporali usando CNN e RNN\n",
        "\n",
        "**Cosa Impareremo:** \n",
        "- Risolvere un tipico problema di classificazione delle serie temporali per casi d'uso industriali.\n",
        "- Caricare, visualizzare e standardizzare dati strutturati (serie temporali)\n",
        "- Costruire, addestrare e valutare modelli di CNN e RNN\n",
        "- Analizzare la curva di apprendimento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12q45gyhCYzM"
      },
      "source": [
        "## Introduzione\n",
        "\n",
        "Questo Lab è un'introduzione alla previsione di serie temporali utilizzando Keras-TensorFlow.\n",
        "\n",
        "Siamo interessati ad addestrare da zero un classificatore di serie temporali sul set di dati FordA  per un caso d'uso industriale."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0R4XBZUCYzO"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwF3ad0GCYzP"
      },
      "source": [
        "## Load del Dataset FordA\n",
        "\n",
        "Il set di dati che stiamo usando qui si chiama FordA ([dettagli](http://www.j-wichard.de/publications/FordPaper.pdf)).\n",
        "Contiene 3601 istanze di training e altre 1320 istanze di test. \n",
        "Ogni serie temporale corrisponde a una misurazione del rumore del motore catturata da un sensore. \n",
        "\n",
        "Problema: classificazione binaria.\n",
        "Lo scenario considerato è di un industria automobilistica che vuole effettuare dei test con obiettivo rilevare automaticamente la presenza di un problema specifico con il motore.\n",
        "\n",
        "Useremo `FordA_TRAIN` file per training e \n",
        "`FordA_TEST` file per testing. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NPL7IVhFCYzQ"
      },
      "outputs": [],
      "source": [
        "def readucr(filename):\n",
        "    data = np.loadtxt(filename, delimiter=\"\\t\")\n",
        "    y = data[:, 0]\n",
        "    x = data[:, 1:]\n",
        "    return x, y.astype(int)\n",
        "\n",
        "\n",
        "root_url = \"https://raw.githubusercontent.com/hfawaz/cd-diagram/master/FordA/\"\n",
        "\n",
        "x_train, y_train = readucr(root_url + \"FordA_TRAIN.tsv\")\n",
        "x_test, y_test = readucr(root_url + \"FordA_TEST.tsv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vediamo che cosa abbiamo sotto mano "
      ],
      "metadata": {
        "id": "XAh8r-UPJDGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train.shape, y_train.shape, np.min(y_train), np.max(y_train))\n",
        "print(x_test.shape, y_test.shape, np.min(x_test), np.max(y_test))"
      ],
      "metadata": {
        "id": "vbhbnNmOIyOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ai fini del laboratorio, riduciamo la dimenzione sia del tr che del test"
      ],
      "metadata": {
        "id": "v-IpCybNJLwA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, y_train = x_train[0:1200], y_train[0:1200]\n",
        "x_test, y_test = x_test[0:500], y_test[0:500]"
      ],
      "metadata": {
        "id": "ektHdTOCe3d6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train.shape, y_train.shape, np.min(y_train), np.max(y_train))\n",
        "print(x_test.shape, y_test.shape, np.min(x_test), np.max(y_test))"
      ],
      "metadata": {
        "id": "B3gc7iFEhVhQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uu0NVGpgCYzQ"
      },
      "source": [
        "## Visualizziamo i dati\n",
        "\n",
        "Qui visualizziamo un esempio di serie temporali per ogni classe nel set di dati\n",
        "\n",
        "Il modo migliore per visualizzare questi dati è l'uso di grafici (plot)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YP4p6YSQCYzR"
      },
      "outputs": [],
      "source": [
        "classes = np.unique(np.concatenate((y_train, y_test), axis=0))\n",
        "\n",
        "plt.figure()\n",
        "for c in classes:\n",
        "    c_x_train = x_train[y_train == c]\n",
        "    plt.plot(c_x_train[0], label=\"class \" + str(c))\n",
        "plt.legend(loc=\"best\")\n",
        "plt.show()\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-JBQwVaCYzS"
      },
      "source": [
        "## Standardizzare (normalizzare) i dati\n",
        "\n",
        "Le nostre serie temporali sono già in un'unica lunghezza (500). \n",
        "Tuttavia, i loro valori sono di solito in vari intervalli. Questo non è l'ideale per una rete neurale; in generale dovremmo cercare di normalizzare i valori di input.\n",
        "\n",
        "Per questo specifico set di dati, i dati sono già z-normalizzati: ogni campione di serie temporali ha una media uguale a zero e una deviazione standard uguale a uno. *Questo tipo di normalizzazione è molto comune per i problemi di classificazione delle serie temporali.*\n",
        "\n",
        "Si noti che i dati delle serie temporali utilizzati qui sono univariati, il che significa che abbiamo un solo canale per esempio di serie temporali. Trasformeremo quindi la serie temporale in una multivariata ad un canale utilizzando un semplice rimodellamento tramite numpy. Questo ci permetterà di costruire un modello facilmente applicabile al tempo multivariato serie."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eW-aNSE1CYzS"
      },
      "outputs": [],
      "source": [
        "x_train = x_train.reshape((x_train.shape[0], x_train.shape[1], 1))\n",
        "x_test = x_test.reshape((x_test.shape[0], x_test.shape[1], 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Per usare `sparse_categorical_crossentropy`, dovremo contare il numero delle classi in anticipo."
      ],
      "metadata": {
        "id": "u55qloCVLAHo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GGUPWH4yCYzU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b94a5410-68f2-4946-8a6e-a7bb7143ab04"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "num_classes = len(np.unique(y_train)) # \n",
        "num_classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UW9eUcECYzU"
      },
      "source": [
        "Ora mescoliamo il dataset perché useremo l'opzione `validation_split`durante l'allenamento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LMkJK7h0CYzU"
      },
      "outputs": [],
      "source": [
        "idx = np.random.permutation(len(x_train))\n",
        "x_train = x_train[idx]\n",
        "y_train = y_train[idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMy5aoslCYzV"
      },
      "source": [
        "Normalizza le etichette su numeri interi positivi. Le etichette previste saranno quindi 0 e 1.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t7tumdolCYzV"
      },
      "outputs": [],
      "source": [
        "y_train[y_train == -1] = 0\n",
        "y_test[y_test == -1] = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPYtHEkNCYzV"
      },
      "source": [
        "## Costruiamo il modello CNN\n",
        "\n",
        "Adesso costruiremo una Convolutional Neural Network (CNN).\n",
        "\n",
        "Qui un blocco convoluzionale si basa su tre componenti principali:\n",
        "1. Un filtro convoluzionale unidimensionale;\n",
        "2. [Opzionale] Un layer di `batch normalization`\n",
        "3. Un layer di pooling\n",
        "\n",
        "I seguenti iperparametri (kernel_size, filters, utilizzo di BatchNorm) sono stati trovati tramite random search utilizzando [KerasTuner](https://github.com/keras-team/keras-tuner).\n",
        "\n",
        "Vediamo che cosa fanno.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "saSzEUi2CYzW"
      },
      "outputs": [],
      "source": [
        "def make_model(input_shape):\n",
        "    input_layer = keras.layers.Input(input_shape)\n",
        "\n",
        "    conv1 = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"same\")(input_layer)\n",
        "    conv1 = keras.layers.BatchNormalization()(conv1)\n",
        "    conv1 = keras.layers.ReLU()(conv1)\n",
        "\n",
        "    conv2 = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"same\")(conv1)\n",
        "    conv2 = keras.layers.BatchNormalization()(conv2)\n",
        "    conv2 = keras.layers.ReLU()(conv2)\n",
        "\n",
        "    conv3 = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"same\")(conv2)\n",
        "    conv3 = keras.layers.BatchNormalization()(conv3)\n",
        "    conv3 = keras.layers.ReLU()(conv3)\n",
        "\n",
        "    gap = keras.layers.GlobalAveragePooling1D()(conv3)\n",
        "\n",
        "    output_layer = keras.layers.Dense(num_classes, activation=\"softmax\")(gap)\n",
        "\n",
        "    return keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "\n",
        "model = make_model(input_shape=x_train.shape[1:])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "keras.utils.plot_model(model, show_shapes=True)"
      ],
      "metadata": {
        "id": "pNpyqKOrPPjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Esercizio** \n",
        "\n",
        "Sai trovare un altro modo per costruire il modello?"
      ],
      "metadata": {
        "id": "tBjTPYeLPBdE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOwQ_GEWCYzW"
      },
      "source": [
        "## Addestriamo il modello"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 50\n",
        "batch_size = 32\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        \"best_model.h5\", save_best_only=True, monitor=\"val_loss\"\n",
        "    ),\n",
        "    keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor=\"val_loss\", factor=0.5, patience=20, min_lr=0.0001\n",
        "    ),\n",
        "    keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=50, verbose=1),\n",
        "]"
      ],
      "metadata": {
        "id": "DJNYg5A3PsiC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VeLfnnhQCYzW"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"sparse_categorical_accuracy\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    callbacks=callbacks,\n",
        "    validation_split=0.25,\n",
        "    verbose=1,\n",
        ")"
      ],
      "metadata": {
        "id": "TLqSZNmdRjxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JoRcGU2tCYzX"
      },
      "source": [
        "## Valutiamo il modello sui dati di test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BPMTmav-CYzX"
      },
      "outputs": [],
      "source": [
        "model = keras.models.load_model(\"best_model.h5\")\n",
        "\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "\n",
        "print(\"Test accuracy\", test_acc)\n",
        "print(\"Test loss\", test_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYXLtXL2CYzX"
      },
      "source": [
        "## Plot della curva di apprendimento \n",
        "\n",
        "Analizziamo la learning curve ottenuta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UGT6x0vICYzX"
      },
      "outputs": [],
      "source": [
        "metric = \"sparse_categorical_accuracy\"\n",
        "plt.figure()\n",
        "plt.plot(history.history[metric])\n",
        "plt.plot(history.history[\"val_\" + metric])\n",
        "plt.title(\"model \" + metric)\n",
        "plt.ylabel(metric, fontsize=\"large\")\n",
        "plt.xlabel(\"epoch\", fontsize=\"large\")\n",
        "plt.legend([\"train\", \"val\"], loc=\"best\")\n",
        "plt.show()\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cosa possiamo osservare?**"
      ],
      "metadata": {
        "id": "IAbWgm15Qmpl"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKxN8ndCCYzX"
      },
      "source": [
        "Possiamo vedere come l'accuratezza nel train raggiunga (?) dopo (?) epoche. \n",
        "Tuttavia, osservando l'accuratezza nel validation, possiamo vedere come la rete (?) \n",
        "\n",
        "Oltre la (?)esima epoca, se continuiamo con l'allenamento, l'accuratezza nel  validation  comincerà a diminuire mentre l'accuratezza nel train continuerà ad aumentare: il modello inizia a \"overfittare\".\n",
        "\n",
        "(?) Studio ed osservazioni degli studenti\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Esercizi (sugeriti)\n",
        "\n",
        "- Sperimentare con lo stesso setup presentato la model selection\n",
        "- Cercare di migliorare le performance (tradeoff con il tempo concesso)\n",
        "- Provare a verificare il fenomeno dell'overfitting\n"
      ],
      "metadata": {
        "id": "v91dQeeoSEDa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Your Turn!\n",
        "\n",
        "Provare con intero dataset FordA questo lab!\n",
        "Quanto tempo impiega? Che risultati sei riuscito ad ottenere?"
      ],
      "metadata": {
        "id": "9BVUMuS_gb2W"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}